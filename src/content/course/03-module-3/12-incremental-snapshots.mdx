---
title: "Incremental Snapshots —á–µ—Ä–µ–∑ Signal Table"
description: "–ö–∞–∫ –≤—ã–ø–æ–ª–Ω—è—Ç—å on-demand snapshot –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü –±–µ–∑ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ connector, –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ signal table, read-only snapshots –¥–ª—è Aurora replicas, –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"
order: 12
difficulty: "advanced"
estimatedTime: 35
topics: ["debezium", "mysql", "aurora", "snapshot", "signal-table", "incremental"]
prerequisites: ["module-8/09-aurora-snapshot-modes"]
---

import { IncrementalSnapshotFlowDiagram, ChunkProcessingDiagram, SignalTableDiagram } from '../../../components/diagrams/module3/IncrementalSnapshotDiagrams';
import Callout from '../../../components/Callout.tsx';

# Incremental Snapshots —á–µ—Ä–µ–∑ Signal Table

## –ö–æ–≥–¥–∞ Initial Snapshot –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ

–í –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —É—Ä–æ–∫–∞—Ö –º—ã –∏–∑—É—á–∏–ª–∏ **initial snapshot** ‚Äî –º–µ—Ö–∞–Ω–∏–∑–º –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ Debezium connector. –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –∑–∞–ø—É—Å–∫–∞, –Ω–æ **–Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç** –¥–ª—è –º–Ω–æ–≥–∏—Ö production —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤:

### –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è Initial Snapshot

**–ü—Ä–æ–±–ª–µ–º–∞ 1: –¢—Ä–µ–±—É–µ—Ç –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ connector**
- Initial snapshot –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è **—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ** (snapshot.mode=initial)
- –ß—Ç–æ–±—ã resnapshot —Ç–∞–±–ª–∏—Ü—É, –Ω—É–∂–Ω–æ **–æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏ —É–¥–∞–ª–∏—Ç—å connector**, –ø–æ—Ç–µ—Ä—è—Ç—å offset
- Downtime –¥–ª—è –≤—Å–µ–≥–æ CDC pipeline –Ω–∞ –≤—Ä–µ–º—è resnapshot

**–ü—Ä–æ–±–ª–µ–º–∞ 2: All-or-nothing –ø–æ–¥—Ö–æ–¥**
- Snapshot –∑–∞—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç **–≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã** –∏–∑ table.include.list
- –ù–µ–ª—å–∑—è resnapshot **–æ–¥–Ω—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Ç–∞–±–ª–∏—Ü—É**, –Ω–µ –∑–∞—Ç—Ä–∞–≥–∏–≤–∞—è –æ—Å—Ç–∞–ª—å–Ω—ã–µ
- –ï—Å–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ A –Ω—É–∂–¥–∞–µ—Ç—Å—è –≤ resnapshot, –ø—Ä–∏–¥–µ—Ç—Å—è resnapshot –∏ —Ç–∞–±–ª–∏—Ü—ã B, C, D

**–ü—Ä–æ–±–ª–µ–º–∞ 3: –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å snapshot –Ω–æ–≤—ã—Ö —Ç–∞–±–ª–∏—Ü**
- –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –≤ table.include.list —Ç—Ä–µ–±—É–µ—Ç connector restart
- Restart —Ç—Ä–∏–≥–≥–µ—Ä–∏—Ç snapshot **–≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü**, –≤–∫–ª—é—á–∞—è —Ç–µ, —á—Ç–æ —É–∂–µ –±—ã–ª–∏ synced

### –†–µ–∞–ª—å–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏, —Ç—Ä–µ–±—É—é—â–∏–µ on-demand snapshot

**–°—Ü–µ–Ω–∞—Ä–∏–π 1: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã**
```
Timeline:
- Day 1: Connector –∑–∞–ø—É—â–µ–Ω —Å —Ç–∞–±–ª–∏—Ü–∞–º–∏ customers, orders
- Day 30: –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–∑–¥–∞–ª–∏ –Ω–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É products
- Task: –î–æ–±–∞–≤–∏—Ç—å products –∫ CDC –±–µ–∑ resnapshot customers, orders
```

**–°—Ü–µ–Ω–∞—Ä–∏–π 2: –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ extended downtime**
```
Scenario:
- Connector –±—ã–ª –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –Ω–∞ 10 –¥–Ω–µ–π (technical issue)
- Binlog retention = 7 –¥–Ω–µ–π (168 —á–∞—Å–æ–≤)
- –†–µ–∑—É–ª—å—Ç–∞—Ç: Binlog files –¥–ª—è –¥–Ω–µ–π 1-3 purged
- Task: Resnapshot –¥–∞–Ω–Ω—ã–µ, –ø–æ—Ç–µ—Ä—è–Ω–Ω—ã–µ –∏–∑-–∑–∞ purge
```

**–°—Ü–µ–Ω–∞—Ä–∏–π 3: Data inconsistency recovery**
```
Issue:
- Consumer application –∏–º–µ–ª bug
- –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–ª UPDATE —Å–æ–±—ã—Ç–∏—è –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã orders
- Target database (Elasticsearch, PostgreSQL) —Å–æ–¥–µ—Ä–∂–∏—Ç corrupted –¥–∞–Ω–Ω—ã–µ
- Task: Resnapshot orders –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è consistency
```

**Incremental snapshot** —Ä–µ—à–∞–µ—Ç –≤—Å–µ —ç—Ç–∏ –ø—Ä–æ–±–ª–µ–º—ã.

## Initial vs Incremental Snapshot: –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ

<IncrementalSnapshotFlowDiagram client:visible />

### –ö–ª—é—á–µ–≤—ã–µ –æ—Ç–ª–∏—á–∏—è

<table style={{fontSize: '0.9em'}}>
  <thead>
    <tr>
      <th>–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞</th>
      <th>Initial Snapshot</th>
      <th>Incremental Snapshot</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>–¢—Ä–∏–≥–≥–µ—Ä</strong></td>
      <td>Connector start (snapshot.mode=initial)</td>
      <td>SQL INSERT –≤ signal table (on-demand)</td>
    </tr>
    <tr>
      <td><strong>Connector restart</strong></td>
      <td>‚úÖ –¢—Ä–µ–±—É–µ—Ç—Å—è</td>
      <td>‚ùå –ù–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è</td>
    </tr>
    <tr>
      <td><strong>Scope</strong></td>
      <td>–í—Å–µ —Ç–∞–±–ª–∏—Ü—ã –∏–∑ table.include.list</td>
      <td>–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã (—É–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –≤ signal)</td>
    </tr>
    <tr>
      <td><strong>–ú–µ—Ç–æ–¥ —á—Ç–µ–Ω–∏—è</strong></td>
      <td>Full table scan –∑–∞ –æ–¥–Ω—É —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é</td>
      <td>Chunk-based (default: 2048 rows/chunk)</td>
    </tr>
    <tr>
      <td><strong>–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å —Å CDC</strong></td>
      <td>‚ùå Binlog streaming blocked –¥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è snapshot</td>
      <td>‚úÖ Binlog streaming –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ</td>
    </tr>
    <tr>
      <td><strong>Locks</strong></td>
      <td>Table locks (minimal mode) –∏–ª–∏ no locks (none mode)</td>
      <td>No locks (chunk reading —á–µ—Ä–µ–∑ SELECT with PK range)</td>
    </tr>
    <tr>
      <td><strong>Resume after failure</strong></td>
      <td>‚ùå Restart from scratch</td>
      <td>‚úÖ Resume from last completed chunk</td>
    </tr>
    <tr>
      <td><strong>Use Case</strong></td>
      <td>–ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ connector</td>
      <td>On-demand resnapshot, –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü, recovery</td>
    </tr>
  </tbody>
</table>

**–ü–æ—á–µ–º—É incremental snapshot –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è "incremental"?**

- –î–∞–Ω–Ω—ã–µ —á–∏—Ç–∞—é—Ç—Å—è **chunk by chunk** (incremental progress)
- –ö–∞–∂–¥—ã–π chunk –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è **–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ**, –º–µ–∂–¥—É chunks Debezium –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç binlog events
- **Resumable**: –ï—Å–ª–∏ snapshot –ø—Ä–µ—Ä—ã–≤–∞–µ—Ç—Å—è, Debezium –∑–Ω–∞–µ—Ç, –∫–∞–∫–∏–µ chunks —É–∂–µ –ø—Ä–æ—á–∏—Ç–∞–Ω—ã

## Signal Table: –ú–µ—Ö–∞–Ω–∏–∑–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è Debezium

Incremental snapshot —Ç—Ä–∏–≥–≥–∏—Ä—É–µ—Ç—Å—è —á–µ—Ä–µ–∑ **signal table** ‚Äî —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é —Å–ª—É–∂–µ–±–Ω—É—é —Ç–∞–±–ª–∏—Ü—É, –∫–æ—Ç–æ—Ä—É—é Debezium –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥.

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Signal Table

<SignalTableDiagram client:visible />

**–ö–ª—é—á–µ–≤–æ–µ –æ—Ç–ª–∏—á–∏–µ –æ—Ç Initial Snapshot:**
- Signal table –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **—Å–∞–º binlog** –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –∫–æ–º–∞–Ω–¥ Debezium
- Debezium —á–∏—Ç–∞–µ—Ç INSERT event –≤ signal table –∫–∞–∫ **trigger –¥–ª—è snapshot**
- –ö–æ–º–∞–Ω–¥–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è **–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ** –≤–º–µ—Å—Ç–µ —Å CDC stream

### Signal Table Schema

Signal table –∏–º–µ–µ—Ç **—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É**, –∫–æ—Ç–æ—Ä—É—é Debezium expects:

```sql
CREATE TABLE debezium_signal (
  id VARCHAR(36) NOT NULL,      -- Unique signal ID (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ PRIMARY KEY)
  type VARCHAR(32) NOT NULL,    -- Signal type: execute-snapshot, stop-snapshot, etc.
  data TEXT,                    -- JSON payload with signal parameters
  PRIMARY KEY (id)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
```

<Callout type="danger" title="PRIMARY KEY –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω - –±–µ–∑ –Ω–µ–≥–æ signal table –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç">

**–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê:** Signal table –±–µ–∑ PRIMARY KEY –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ **silent failure**.

**–ß—Ç–æ –ø—Ä–æ–∏–∑–æ–π–¥–µ—Ç:**
1. –í—ã –≤—ã–ø–æ–ª–Ω—è–µ—Ç–µ INSERT –≤ signal table –±–µ–∑ PK
2. Debezium capture INSERT event –∏–∑ binlog
3. Debezium –ø—ã—Ç–∞–µ—Ç—Å—è parse event ‚Üí **fails silently** (no error logs)
4. Incremental snapshot **–Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è**, –Ω–∏–∫–∞–∫–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–µ

**–ü—Ä–æ–≤–µ—Ä–∫–∞:**
```sql
SHOW CREATE TABLE debezium_signal\G
```

**–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –Ω–∞–ª–∏—á–∏–µ:**
```
PRIMARY KEY (`id`)
```

**–ï—Å–ª–∏ PRIMARY KEY –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç:**
```sql
ALTER TABLE debezium_signal ADD PRIMARY KEY (id);
```

</Callout>

**Column requirements:**

| Column | Type       | Required | Purpose                                      |
|--------|------------|----------|----------------------------------------------|
| `id`   | VARCHAR(36)| ‚úÖ       | Unique ID (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è UUID())             |
| `type` | VARCHAR(32)| ‚úÖ       | Signal type: execute-snapshot, stop-snapshot |
| `data` | TEXT       | ‚úÖ       | JSON payload (–º–æ–∂–µ—Ç –±—ã—Ç—å NULL –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö signals) |

**–°–æ–∑–¥–∞–Ω–∏–µ signal table:**

```sql
-- –ü–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ MySQL
mysql -h localhost -P 3307 -u root -pmysql inventory

-- –°–æ–∑–¥–∞—Ç—å signal table
CREATE TABLE debezium_signal (
  id VARCHAR(36) NOT NULL,
  type VARCHAR(32) NOT NULL,
  data TEXT,
  PRIMARY KEY (id)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

-- –í—ã–¥–∞—Ç—å –ø—Ä–∞–≤–∞ debezium user
GRANT INSERT, UPDATE, DELETE ON inventory.debezium_signal TO 'debezium'@'%';
FLUSH PRIVILEGES;
```

**–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è:**

```sql
DESCRIBE debezium_signal;

-- Expected output:
-- +-------+-------------+------+-----+---------+-------+
-- | Field | Type        | Null | Key | Default | Extra |
-- +-------+-------------+------+-----+---------+-------+
-- | id    | varchar(36) | NO   | PRI | NULL    |       |
-- | type  | varchar(32) | NO   |     | NULL    |       |
-- | data  | text        | YES  |     | NULL    |       |
-- +-------+-------------+------+-----+---------+-------+
```

<Callout type="tip" title="UUID() –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö signal IDs">

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ MySQL —Ñ—É–Ω–∫—Ü–∏—é `UUID()` –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö IDs:

```sql
INSERT INTO debezium_signal (id, type, data)
VALUES (UUID(), 'execute-snapshot', '...');
```

**–ü–æ—á–µ–º—É UUID()?**
- –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç uniqueness –¥–∞–∂–µ –ø—Ä–∏ concurrent signals
- –ù–µ —Ç—Ä–µ–±—É–µ—Ç manual tracking –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ ID
- Compatible —Å distributed systems (–Ω–µ—Å–∫–æ–ª—å–∫–æ engineers –º–æ–≥—É—Ç –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å signals –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã:**
- Sequential IDs: `'signal-001'`, `'signal-002'` (—Ç—Ä–µ–±—É–µ—Ç coordination)
- Timestamp-based: `CONCAT('sig-', UNIX_TIMESTAMP())` (–º–æ–∂–µ—Ç –∏–º–µ—Ç—å collisions)

</Callout>

## Connector Configuration –¥–ª—è Signal Table

–ß—Ç–æ–±—ã Debezium –Ω–∞—á–∞–ª –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å signal table, –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Å–≤–æ–π—Å—Ç–≤–æ `signal.data.collection` –≤ connector config:

```json
{
  "name": "mysql-inventory-connector",
  "config": {
    "connector.class": "io.debezium.connector.mysql.MySqlConnector",
    "database.hostname": "mysql",
    "database.port": "3306",
    "database.user": "debezium",
    "database.password": "dbz",
    "database.server.id": "184054",

    // Table filtering
    "database.include.list": "inventory",
    "table.include.list": "inventory.customers,inventory.orders,inventory.products",

    // Topic naming
    "topic.prefix": "mysql-server",

    // SIGNAL TABLE CONFIGURATION
    "signal.data.collection": "inventory.debezium_signal",

    // Incremental snapshot chunk size (optional, default: 2048)
    "incremental.snapshot.chunk.size": "2048",

    // Schema history
    "schema.history.internal.kafka.bootstrap.servers": "kafka:9092",
    "schema.history.internal.kafka.topic": "schema-changes.mysql-server",

    // Snapshot mode
    "snapshot.mode": "initial",

    // Other properties...
    "heartbeat.interval.ms": "30000",
    "decimal.handling.mode": "precise"
  }
}
```

**–ö–ª—é—á–µ–≤—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞:**

### 1. signal.data.collection

```json
"signal.data.collection": "inventory.debezium_signal"
```

**–ß—Ç–æ —ç—Ç–æ:** Fully-qualified name signal table (format: `{database}.{table}`).

**–í–∞–∂–Ω–æ:**
- Signal table schema **–î–û–õ–ñ–ù–ê –±—ã—Ç—å –≤–∫–ª—é—á–µ–Ω–∞** –≤ `database.include.list`
- –ï—Å–ª–∏ signal table –≤ –¥—Ä—É–≥–æ–π database, –¥–æ–±–∞–≤—å—Ç–µ database –≤ whitelist
- Debezium –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç INSERT events –≤ —ç—Ç—É —Ç–∞–±–ª–∏—Ü—É —á–µ—Ä–µ–∑ binlog

**–ü—Ä–∏–º–µ—Ä —Å signal table –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π database:**

```json
"database.include.list": "inventory,debezium_admin",
"signal.data.collection": "debezium_admin.signal_table"
```

### 2. incremental.snapshot.chunk.size

```json
"incremental.snapshot.chunk.size": "2048"
```

**–ß—Ç–æ —ç—Ç–æ:** –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ rows, —á–∏—Ç–∞–µ–º—ã—Ö –∑–∞ –æ–¥–∏–Ω chunk.

**–ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ:**

| Chunk Size | When to Use                          | Trade-offs                        |
|------------|--------------------------------------|-----------------------------------|
| 1024       | –¢–∞–±–ª–∏—Ü—ã —Å –æ—á–µ–Ω—å —à–∏—Ä–æ–∫–∏–º–∏ rows (100+ columns) | –ú–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ –º–µ–Ω—å—à–µ memory usage |
| 2048       | **Default** - –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ cases | Balanced performance              |
| 4096       | –¢–∞–±–ª–∏—Ü—ã —Å —É–∑–∫–∏–º–∏ rows (5-10 columns) | –ë—ã—Å—Ç—Ä–µ–µ, –Ω–æ –±–æ–ª—å—à–µ memory         |
| 8192       | Large tables, high-performance snapshot | –ú–æ–∂–µ—Ç –≤—ã–∑–≤–∞—Ç—å memory pressure     |

**Formula –¥–ª—è –æ—Ü–µ–Ω–∫–∏ chunk size:**

```
Chunk Size = (Desired Memory per Chunk MB) / (Average Row Size KB) * 1024
```

**–ü—Ä–∏–º–µ—Ä:**
- Average row size: 5 KB
- Desired memory per chunk: 10 MB
- Chunk size = 10 MB / 5 KB = 2048 rows

<Callout type="warning" title="Signal table schema –î–û–õ–ñ–ù–ê –±—ã—Ç—å –≤ database.include.list">

**–ß–ê–°–¢–ê–Ø –û–®–ò–ë–ö–ê:**

```json
"database.include.list": "inventory",
"signal.data.collection": "inventory.debezium_signal"
```

–ï—Å–ª–∏ `inventory` –≤ `database.include.list` ‚Äî ‚úÖ —Ä–∞–±–æ—Ç–∞–µ—Ç.

**–ù–û –µ—Å–ª–∏:**

```json
"database.include.list": "orders_db",
"signal.data.collection": "admin.debezium_signal"
```

Signal table –≤ `admin` database, –∫–æ—Ç–æ—Ä–∞—è **–ù–ï –≤ whitelist** ‚Üí signals **–Ω–µ –±—É–¥—É—Ç captured**.

**–†–µ—à–µ–Ω–∏–µ:**

```json
"database.include.list": "orders_db,admin"
```

**Verification:**
–ü–æ—Å–ª–µ deployment connector, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ logs:

```bash
docker compose logs debezium-connect | grep "signal.data.collection"
```

–û–∂–∏–¥–∞–µ–º—ã–π log:
```
INFO: Signal data collection set to 'inventory.debezium_signal'
```

–ï—Å–ª–∏ –Ω–µ—Ç —ç—Ç–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è ‚Äî –ø—Ä–æ–≤–µ—Ä—å—Ç–µ whitelist.

</Callout>

## Triggering Incremental Snapshot: SQL Examples

–¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ signal table –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ –∏ connector –∑–Ω–∞–µ—Ç –æ –Ω–µ–π, –º–æ–∂–Ω–æ trigger incremental snapshot —á–µ—Ä–µ–∑ SQL INSERT.

### –ë–∞–∑–æ–≤—ã–π Snapshot: –û–¥–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞

```sql
-- Snapshot —Ç–∞–±–ª–∏—Ü—ã inventory.orders
INSERT INTO inventory.debezium_signal (id, type, data)
VALUES (
  UUID(),
  'execute-snapshot',
  '{"data-collections": ["inventory.orders"]}'
);
```

**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç:**
1. Debezium capture INSERT event –∏–∑ binlog
2. Parse `type` field: `execute-snapshot`
3. Parse `data` JSON: `data-collections = ["inventory.orders"]`
4. Start incremental snapshot –¥–ª—è `inventory.orders`
5. Chunk-based reading: SELECT * FROM orders WHERE id >= X AND id < X+2048
6. –ú–µ–∂–¥—É chunks –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç binlog events (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)
7. –ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö chunks: snapshot complete

**Expected connector logs:**

```
INFO: Incremental snapshot requested for data collections: [inventory.orders]
INFO: Starting incremental snapshot for table 'inventory.orders'
INFO: Snapshot window for table 'inventory.orders': [1, 2048]
INFO: Snapshot window for table 'inventory.orders': [2049, 4096]
...
INFO: Incremental snapshot completed for table 'inventory.orders'
```

### Snapshot –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–∞–±–ª–∏—Ü

```sql
-- Snapshot —Ç—Ä–µ—Ö —Ç–∞–±–ª–∏—Ü –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
INSERT INTO inventory.debezium_signal (id, type, data)
VALUES (
  UUID(),
  'execute-snapshot',
  '{"data-collections": ["inventory.customers", "inventory.orders", "inventory.products"]}'
);
```

**–û–±—Ä–∞–±–æ—Ç–∫–∞:**
- Debezium –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã **–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ** (–Ω–µ parallel)
- –ü–æ—Ä—è–¥–æ–∫: customers ‚Üí orders ‚Üí products
- –ö–∞–∂–¥–∞—è —Ç–∞–±–ª–∏—Ü–∞ chunk-based
- Binlog streaming –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü

### Filtered Snapshot: Snapshot —Å WHERE —É—Å–ª–æ–≤–∏–µ–º

```sql
-- Snapshot —Ç–æ–ª—å–∫–æ orders —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –≤ 2024 –≥–æ–¥—É
INSERT INTO inventory.debezium_signal (id, type, data)
VALUES (
  UUID(),
  'execute-snapshot',
  '{
    "data-collections": ["inventory.orders"],
    "additional-conditions": [
      {
        "data-collection": "inventory.orders",
        "filter": "created_at >= ''2024-01-01'' AND created_at < ''2025-01-01''"
      }
    ]
  }'
);
```

**–í–∞–∂–Ω–æ:**
- `filter` field —Å–æ–¥–µ—Ä–∂–∏—Ç SQL WHERE condition
- **Double quotes escaping:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `''` (–¥–≤–∞ –æ–¥–∏–Ω–∞—Ä–Ω—ã—Ö quote) –≤–º–µ—Å—Ç–æ `'` –¥–ª—è –ª–∏—Ç–µ—Ä–∞–ª–æ–≤ –≤ JSON
- –£—Å–ª–æ–≤–∏–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ –∫–∞–∂–¥–æ–º—É chunk: `SELECT * WHERE (chunk condition) AND (filter condition)`

**Use cases:**
- Snapshot —Ç–æ–ª—å–∫–æ recent data (last 30 days)
- Snapshot specific partition (e.g., region = 'US-EAST')
- Snapshot failed rows (e.g., sync_status = 'FAILED')

### Stop In-Progress Snapshot

```sql
-- –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ç–µ–∫—É—â–∏–π incremental snapshot
INSERT INTO inventory.debezium_signal (id, type, data)
VALUES (
  UUID(),
  'stop-snapshot',
  '{"data-collections": ["inventory.orders"], "type": "INCREMENTAL"}'
);
```

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
- Snapshot –∑–∞–Ω–∏–º–∞–µ—Ç —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ (–±–ª–æ–∫–∏—Ä—É–µ—Ç –¥—Ä—É–≥–∏–µ operations)
- Discovered –æ—à–∏–±–∫–∞ –≤ filter condition (–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ snapshot)
- Need to reconfigure chunk size

**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç:**
- Debezium –∑–∞–≤–µ—Ä—à–∞–µ—Ç —Ç–µ–∫—É—â–∏–π chunk
- –ü—Ä–µ–∫—Ä–∞—â–∞–µ—Ç snapshot –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è chunk (–Ω–µ –ø—Ä–µ—Ä—ã–≤–∞–µ—Ç chunk mid-way)
- Events —É–∂–µ sent to Kafka ‚Äî –æ—Å—Ç–∞—é—Ç—Å—è (–Ω–µ –æ—Ç–∫–∞—Ç—ã–≤–∞—é—Ç—Å—è)

**Resume snapshot:**
- Incremental snapshot **–ù–ï resumable** –ø–æ—Å–ª–µ stop-snapshot
- –ù—É–∂–Ω–æ trigger –Ω–æ–≤—ã–π execute-snapshot signal

## Snapshot Progress Monitoring

Incremental snapshot –º–æ–∂–µ—Ç –¥–ª–∏—Ç—å—Å—è **—á–∞—Å—ã** –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ç–∞–±–ª–∏—Ü. –í–∞–∂–Ω–æ –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å.

### JMX Metrics: Real-Time Status

Debezium connector expose JMX metrics –¥–ª—è incremental snapshot:

**Key Metrics:**

| Metric                        | Type    | Meaning                                   |
|-------------------------------|---------|-------------------------------------------|
| `SnapshotRunning`             | Boolean | `true` –µ—Å–ª–∏ incremental snapshot –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ |
| `SnapshotCompleted`           | Boolean | `true` –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è snapshot          |
| `SnapshotAborted`             | Boolean | `true` –µ—Å–ª–∏ snapshot stopped/failed       |
| `ChunkId`                     | String  | Current chunk ID (e.g., "1:2048")         |
| `TotalTableCount`             | Integer | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∞–±–ª–∏—Ü –≤ snapshot              |
| `RemainingTableCount`         | Integer | –û—Å—Ç–∞–≤—à–∏–µ—Å—è —Ç–∞–±–ª–∏—Ü—ã                        |

**Access JMX metrics via JConsole:**

```bash
# Enable JMX in Kafka Connect (docker-compose.yml)
environment:
  KAFKA_JMX_OPTS: "-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.port=9999 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false"

# Connect JConsole to localhost:9999
# Navigate to: debezium.mysql:type=connector-metrics,context=snapshot,server=mysql-server
```

### Connector Logs: Detailed Progress

```bash
# Real-time logs
docker compose logs -f debezium-connect | grep -E "(Incremental|Snapshot|chunk)"

# Expected output:
# INFO: Incremental snapshot requested for data collections: [inventory.orders]
# INFO: Snapshot window for table 'inventory.orders': [1, 2048]
# INFO: Snapshot - Scanned 2048 rows from table 'inventory.orders'
# INFO: Snapshot window for table 'inventory.orders': [2049, 4096]
# INFO: Snapshot - Scanned 2048 rows from table 'inventory.orders'
# ...
# INFO: Incremental snapshot completed for table 'inventory.orders'
```

**–ö–∞–∫ –æ—Ü–µ–Ω–∏—Ç—å –æ—Å—Ç–∞–≤—à–µ–µ—Å—è –≤—Ä–µ–º—è:**

```python
# Pseudo-code –¥–ª—è estimation
total_rows = 1000000  # –ò–∑ SHOW TABLE STATUS
chunk_size = 2048
avg_chunk_duration_sec = 5  # –ó–∞—Å–µ–∫–∞–µ–º –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ chunk –∏–∑ logs

chunks_remaining = (total_rows - current_row) / chunk_size
estimated_time_sec = chunks_remaining * avg_chunk_duration_sec
estimated_hours = estimated_time_sec / 3600
```

### Prometheus Queries (–µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω JMX Exporter)

```promql
# Snapshot running status
debezium_metrics_SnapshotRunning{context="snapshot",server="mysql-server"}

# Remaining table count
debezium_metrics_RemainingTableCount{context="snapshot",server="mysql-server"}

# Alert if snapshot running > 6 hours
(
  debezium_metrics_SnapshotRunning == 1
  and
  time() - debezium_metrics_SnapshotStartTimestamp > 21600
)
```

**Grafana Panel Example:**

```json
{
  "title": "Incremental Snapshot Progress",
  "targets": [
    {
      "expr": "debezium_metrics_RemainingTableCount{server=\"mysql-server\"}",
      "legendFormat": "Remaining Tables"
    }
  ],
  "yaxes": [
    {
      "label": "Table Count",
      "min": 0
    }
  ]
}
```

<Callout type="tip" title="Grafana –ø–∞–Ω–µ–ª–∏ –∏–∑ Phase 15 Plan 01">

–í **Phase 15 Plan 01** –º—ã —Å–æ–∑–¥–∞–ª–∏ Grafana dashboards –¥–ª—è Debezium monitoring. –î–ª—è incremental snapshot —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å –ø–∞–Ω–µ–ª—å:

**Snapshot Status Panel:**
- Metric: `debezium_metrics_SnapshotRunning`
- Visualization: Stat panel (–ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç "Running" / "Idle")
- Alert: –ï—Å–ª–∏ snapshot running > 12 —á–∞—Å–æ–≤

**Snapshot Progress Panel:**
- Metric: `debezium_metrics_RemainingTableCount`
- Visualization: Graph (line chart showing decline to 0)
- Helpful –¥–ª—è tracking multi-table snapshots

</Callout>

## Read-Only Incremental Snapshots –¥–ª—è Aurora Replicas

–í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö production —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö CDC connector –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ **Aurora read replica**, –∞ –Ω–µ –∫ writer instance:

**–ó–∞—á–µ–º CDC —Å read replica?**
- **Offload** snapshot –∏ binlog reading —Å production writer
- **Zero impact** –Ω–∞ write latency –¥–ª—è application
- **Compliance**: Read-only access policy (Debezium user –Ω–µ –º–æ–∂–µ—Ç –ø–∏—Å–∞—Ç—å –≤ production)

**–ü—Ä–æ–±–ª–µ–º–∞:** Signal table —Ç—Ä–µ–±—É–µ—Ç **INSERT** –∫–æ–º–∞–Ω–¥—ã ‚Üí **–Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –Ω–∞ read-only replica**.

**–†–µ—à–µ–Ω–∏–µ:** **Kafka Signal Channel** ‚Äî alternative –º–µ—Ö–∞–Ω–∏–∑–º signaling –±–µ–∑ database writes.

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Read-Only Incremental Snapshot

<ChunkProcessingDiagram client:visible />

**Workflow –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è:**
1. **Signal table –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è** (replica read-only)
2. **Kafka signal topic** —Å–æ–∑–¥–∞–µ—Ç—Å—è –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ commands
3. Engineer sends signals via `kafka-console-producer` –≤–º–µ—Å—Ç–æ SQL INSERT
4. Debezium connector consumes signals from Kafka topic

### Prerequisites –¥–ª—è Read-Only Snapshot

<Callout type="danger" title="GTID prerequisites –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã –¥–ª—è read-only snapshot">

Read-only incremental snapshot —Ç—Ä–µ–±—É–µ—Ç **GTID mode ON** –∏ **–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ GTID settings** –Ω–∞ Aurora:

**–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**

```sql
-- –í Aurora DB Cluster Parameter Group
gtid_mode = ON
enforce_gtid_consistency = ON
replica_preserve_commit_order = ON  -- CRITICAL –¥–ª—è read replicas
```

**–ü–æ—á–µ–º—É `replica_preserve_commit_order` –∫—Ä–∏—Ç–∏—á–µ–Ω?**

- Debezium —á–∏—Ç–∞–µ—Ç binlog —Å **read replica**, –≥–¥–µ events replication asynchronous
- –ë–µ–∑ `replica_preserve_commit_order`: Binlog events –º–æ–≥—É—Ç –ø–æ—è–≤–∏—Ç—å—Å—è **out of order** –Ω–∞ replica
- Debezium –º–æ–∂–µ—Ç –ø—Ä–æ—á–∏—Ç–∞—Ç—å UPDATE event **–¥–æ** —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ INSERT event
- –†–µ–∑—É–ª—å—Ç–∞—Ç: Data inconsistency –≤ Kafka topics

**–ü—Ä–æ–≤–µ—Ä–∫–∞ prerequisites:**

```sql
-- –ü–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Aurora read replica
mysql -h aurora-replica.cluster-ro-abc123.us-east-1.rds.amazonaws.com -u debezium -p

-- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å GTID mode
SHOW VARIABLES LIKE 'gtid_mode';
-- Expected: ON

-- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å enforce_gtid_consistency
SHOW VARIABLES LIKE 'enforce_gtid_consistency';
-- Expected: ON

-- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å replica_preserve_commit_order (Aurora 2.10+)
SHOW VARIABLES LIKE 'replica_preserve_commit_order';
-- Expected: ON
```

**–ï—Å–ª–∏ settings –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã:**
- –û–±–Ω–æ–≤–∏—Ç—å DB Cluster Parameter Group (–Ω–µ DB Parameter Group!)
- Reboot **read replica instances** (writer –º–æ–∂–µ—Ç –æ—Å—Ç–∞—Ç—å—Å—è running)

</Callout>

### Kafka Signal Topic Setup

**Step 1: –°–æ–∑–¥–∞—Ç—å Kafka signal topic**

```bash
docker compose exec kafka kafka-topics \
  --bootstrap-server localhost:9092 \
  --create \
  --topic debezium-signal-mysql \
  --partitions 1 \
  --replication-factor 3 \
  --config retention.ms=86400000  # 24 hours (signals ephemeral)
```

**Topic configuration:**

| Parameter           | Value | Reasoning                                    |
|---------------------|-------|----------------------------------------------|
| `--partitions`      | 1     | Single partition –¥–ª—è ordered signal processing |
| `--replication-factor` | 3  | High availability (production)               |
| `retention.ms`      | 86400000 | 24 hours (signals –º–æ–∂–Ω–æ purge –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏) |

### Connector Configuration –¥–ª—è Kafka Signal Channel

```json
{
  "name": "aurora-replica-cdc-connector",
  "config": {
    "connector.class": "io.debezium.connector.mysql.MySqlConnector",

    // CONNECT TO READ REPLICA (not writer)
    "database.hostname": "aurora-cluster.cluster-ro-abc123.us-east-1.rds.amazonaws.com",
    "database.port": "3306",
    "database.user": "debezium",
    "database.password": "${file:/secrets/mysql-password.txt:password}",
    "database.server.id": "184057",

    "database.include.list": "inventory",
    "table.include.list": "inventory.customers,inventory.orders",
    "topic.prefix": "aurora-mysql",

    // KAFKA SIGNAL CHANNEL (instead of signal.data.collection)
    "signal.kafka.topic": "debezium-signal-mysql",
    "signal.kafka.bootstrap.servers": "kafka:9092",
    "signal.kafka.groupId": "debezium-signal-consumer-aurora",

    // Incremental snapshot chunk size
    "incremental.snapshot.chunk.size": "2048",

    // Schema history
    "schema.history.internal.kafka.bootstrap.servers": "kafka:9092",
    "schema.history.internal.kafka.topic": "schema-changes.aurora-mysql",

    // Snapshot mode
    "snapshot.mode": "initial",

    // GTID source (critical for read replicas)
    "gtid.source.includes": ".*",

    // Other properties
    "heartbeat.interval.ms": "30000",
    "decimal.handling.mode": "precise"
  }
}
```

**–ö–ª—é—á–µ–≤—ã–µ –æ—Ç–ª–∏—á–∏—è –æ—Ç database signal table:**

```diff
- "signal.data.collection": "inventory.debezium_signal"
+ "signal.kafka.topic": "debezium-signal-mysql"
+ "signal.kafka.bootstrap.servers": "kafka:9092"
+ "signal.kafka.groupId": "debezium-signal-consumer-aurora"
```

### Triggering Snapshot —á–µ—Ä–µ–∑ Kafka Signal Topic

**–í–º–µ—Å—Ç–æ SQL INSERT:**

```sql
-- ‚ùå –ù–ï —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ read-only replica
INSERT INTO debezium_signal (id, type, data) VALUES (...);
```

**–ò—Å–ø–æ–ª—å–∑—É–µ–º kafka-console-producer:**

```bash
# Trigger incremental snapshot –¥–ª—è inventory.orders
docker compose exec kafka kafka-console-producer \
  --bootstrap-server localhost:9092 \
  --topic debezium-signal-mysql \
  --property "parse.key=true" \
  --property "key.separator=:"

# –í –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ä–µ–∂–∏–º–µ –≤–≤–µ–¥–∏—Ç–µ (–æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞):
{"id":"aurora-snapshot-001"}:{"type":"execute-snapshot","data":{"data-collections":["inventory.orders"]}}
```

**Format signal message:**

```
Key: {"id": "unique-signal-id"}
Value: {"type": "execute-snapshot", "data": {"data-collections": ["inventory.orders"]}}
```

**Key components:**

- **Key:** JSON —Å –ø–æ–ª–µ–º `id` (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å unique)
- **Value:** JSON —Å –ø–æ–ª—è–º–∏ `type` –∏ `data`
- **Separator:** `:` (–Ω–∞—Å—Ç—Ä–æ–µ–Ω —á–µ—Ä–µ–∑ `key.separator`)

**Filtered snapshot —á–µ—Ä–µ–∑ Kafka:**

```bash
docker compose exec kafka kafka-console-producer \
  --bootstrap-server localhost:9092 \
  --topic debezium-signal-mysql \
  --property "parse.key=true" \
  --property "key.separator=:"

# Input:
{"id":"aurora-snapshot-002"}:{"type":"execute-snapshot","data":{"data-collections":["inventory.orders"],"additional-conditions":[{"data-collection":"inventory.orders","filter":"created_at >= '2024-01-01'"}]}}
```

**Stop snapshot —á–µ—Ä–µ–∑ Kafka:**

```bash
{"id":"aurora-stop-001"}:{"type":"stop-snapshot","data":{"data-collections":["inventory.orders"],"type":"INCREMENTAL"}}
```

<Callout type="warning" title="–ü–æ—á–µ–º—É regular snapshots –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–∞ read-only replicas?">

**–í–æ–ø—Ä–æ—Å:** –ï—Å–ª–∏ signal table –ø—Ä–æ–±–ª–µ–º–∞ –¥–ª—è read-only replicas, –ø–æ—á–µ–º—É –Ω–µ–ª—å–∑—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **initial snapshot** (snapshot.mode=initial)?

**–û—Ç–≤–µ—Ç:** Initial snapshot **—Ç–æ–∂–µ —Ç—Ä–µ–±—É–µ—Ç write access** –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö (–Ω–µ –¥–ª—è data, –Ω–æ –¥–ª—è internal state management):

**Initial snapshot steps requiring writes:**
1. **Locking phase:** `LOCK TABLES` (read lock, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç LOCK privilege)
2. **GTID position recording:** –ù–µ–∫–æ—Ç–æ—Ä—ã–µ Debezium versions –ø—ã—Ç–∞—é—Ç—Å—è –∑–∞–ø–∏—Å–∞—Ç—å metadata
3. **Snapshot marker:** Debezium –º–æ–∂–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å temporary tables –¥–ª—è tracking snapshot state

**Aurora read replica restrictions:**
- `LOCK TABLES` –º–æ–∂–µ—Ç –±—ã—Ç—å blocked (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç Aurora version –∏ parameter groups)
- –õ—é–±—ã–µ DDL statements (CREATE TABLE, –¥–∞–∂–µ temporary) ‚Äî denied
- User —Å read-only grants –Ω–µ –º–æ–∂–µ—Ç –≤—ã–ø–æ–ª–Ω–∏—Ç—å LOCK operations

**–†–µ—à–µ–Ω–∏–µ:**
- Initial snapshot –∑–∞–ø—É—Å–∫–∞—Ç—å –Ω–∞ **writer instance**
- Incremental snapshots –Ω–∞ **read replica** —á–µ—Ä–µ–∑ Kafka signal channel
- –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å backup-based approach (Phase 14 Lesson 09)

</Callout>

## Common Signal Table Pitfalls

### Pitfall 1: Signal Table –±–µ–∑ database.include.list

**Scenario:**

```json
"database.include.list": "inventory",
"signal.data.collection": "admin.debezium_signal"
```

Signal table –≤ database `admin`, –∫–æ—Ç–æ—Ä–∞—è **–Ω–µ –≤ whitelist**.

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- Debezium **–Ω–µ capture** INSERT events –≤ signal table
- Incremental snapshot **–Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è**
- –ù–µ—Ç error logs (silent failure)

**–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:**

```bash
# Check connector logs –¥–ª—è signal parsing
docker compose logs debezium-connect | grep "signal"

# –ï—Å–ª–∏ –Ω–µ—Ç logs –≤–∏–¥–∞:
# "INFO: Signal data collection set to 'admin.debezium_signal'"
# –ü—Ä–æ–±–ª–µ–º–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞
```

**–†–µ—à–µ–Ω–∏–µ:**

```json
"database.include.list": "inventory,admin"
```

### Pitfall 2: Missing PRIMARY KEY

**Scenario:**

```sql
CREATE TABLE debezium_signal (
  id VARCHAR(36),
  type VARCHAR(32),
  data TEXT
  -- ‚ùå NO PRIMARY KEY
);
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- Debezium capture INSERT events, –Ω–æ **fails to parse** (expects PK –¥–ª—è binlog event structure)
- Silent failure, no snapshot triggered

**–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:**

```sql
SHOW CREATE TABLE debezium_signal\G
```

–ï—Å–ª–∏ output –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç `PRIMARY KEY` ‚Äî –ø—Ä–æ–±–ª–µ–º–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞.

**–†–µ—à–µ–Ω–∏–µ:**

```sql
ALTER TABLE debezium_signal ADD PRIMARY KEY (id);
```

### Pitfall 3: Binlog Retention < Snapshot Duration

**Scenario:**
- Incremental snapshot –¥–ª—è 500GB table –∑–∞–ø—É—â–µ–Ω
- Expected duration: 10 —á–∞—Å–æ–≤
- Binlog retention: 7 –¥–Ω–µ–π (168 —á–∞—Å–æ–≤)
- –ù–æ –∑–∞ –≤—Ä–µ–º—è snapshot –ø—Ä–æ—à–ª–æ 8 –¥–Ω–µ–π (connector –±—ã–ª stopped –∏–∑-–∑–∞ issue)

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- Snapshot –Ω–∞—á–∞–ª—Å—è —Å binlog position `mysql-bin.000120:154`
- –ü–æ—Å–ª–µ 10 —á–∞—Å–æ–≤ snapshot –∑–∞–≤–µ—Ä—à–µ–Ω
- Debezium –ø—ã—Ç–∞–µ—Ç—Å—è resume binlog streaming —Å position `mysql-bin.000120:154`
- –§–∞–π–ª `mysql-bin.000120` **purged** (retention exceeded)
- Error: "Cannot replicate because master purged required binary logs"

**–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:**

```sql
-- Check binlog retention
SHOW VARIABLES LIKE 'binlog_expire_logs_seconds';

-- Check available binlog files
SHOW BINARY LOGS;
```

**Mitigation:**

**Formula:**
```
Required Binlog Retention >= Snapshot Duration + Safety Margin
```

**Example:**
- Snapshot duration estimate: 12 hours
- Safety margin: 2x (for unexpected delays)
- Required retention: 12 * 2 = 24 hours (–º–∏–Ω–∏–º—É–º)

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** **7 days retention** (604800 seconds) –¥–ª—è production.

**Aurora Configuration:**

```sql
-- –í DB Cluster Parameter Group
binlog_expire_logs_seconds = 604800  -- 7 days
```

**Monitor binlog usage during snapshot:**

```sql
-- Check binlog disk usage
SELECT
  SUM(ROUND(file_size / 1024 / 1024, 2)) AS binlog_size_mb
FROM information_schema.innodb_tablespaces
WHERE name LIKE 'mysql-bin%';
```

**Aurora CloudWatch metric:**

```
Metric: ChangeLogBytesUsed
Threshold: < 80% of available storage
```

### Pitfall 4: Read-Only Environment –±–µ–∑ GTID Prerequisites

**Scenario:**
- Incremental snapshot configured –¥–ª—è Aurora read replica
- Kafka signal channel setup
- –ù–æ `replica_preserve_commit_order=OFF`

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- Snapshot –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è
- Debezium —á–∏—Ç–∞–µ—Ç chunks —Å replica
- Binlog events arrive **out of order** –Ω–∞ replica
- Chunk N —Å–æ–¥–µ—Ä–∂–∏—Ç UPDATE –¥–ª—è row ID=500
- –ù–æ INSERT –¥–ª—è row ID=500 –µ—â–µ –Ω–µ —Ä–µ–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω —Å writer
- Debezium publishes UPDATE event –±–µ–∑ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ CREATE event
- Consumer application –≤–∏–¥–∏—Ç UPDATE –¥–ª—è non-existent row ‚Üí **data inconsistency**

**–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:**

```sql
SHOW VARIABLES LIKE 'replica_preserve_commit_order';
-- If OFF or not set ‚Üí problem confirmed
```

**–†–µ—à–µ–Ω–∏–µ:**

```sql
-- –í Aurora DB Cluster Parameter Group
replica_preserve_commit_order = ON

-- Reboot read replica instances
```

**Verification –ø–æ—Å–ª–µ reboot:**

```sql
SHOW VARIABLES LIKE 'replica_preserve_commit_order';
-- Expected: ON
```

## Binlog Retention Planning –¥–ª—è Large Table Snapshots

–î–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö —Ç–∞–±–ª–∏—Ü incremental snapshot –º–æ–∂–µ—Ç –¥–ª–∏—Ç—å—Å—è **–¥–Ω–∏**. –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å, —á—Ç–æ binlog files –¥–æ—Å—Ç—É–ø–Ω—ã –Ω–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ –≤—Å–µ–≥–æ snapshot.

### Estimating Snapshot Duration

**Formula:**

```
Snapshot Duration (hours) = (Total Rows / Chunk Size) √ó Average Chunk Time (seconds) / 3600
```

**Example:**

```python
total_rows = 100_000_000  # 100 million rows
chunk_size = 2048
avg_chunk_time_sec = 2  # 2 seconds per chunk (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç network, table width, indexes)

chunks = total_rows / chunk_size  # 48,828 chunks
total_time_sec = chunks * avg_chunk_time_sec  # 97,656 seconds
total_hours = total_time_sec / 3600  # 27 hours
```

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –ò–∑–º–µ—Ä–∏—Ç—å `avg_chunk_time` –¥–ª—è –ø–µ—Ä–≤—ã—Ö 10 chunks –∏–∑ logs, –∑–∞—Ç–µ–º extrapolate.

### Retention Formula

```
Required Retention >= Snapshot Duration + Safety Margin + Normal Operations Buffer
```

**Components:**

1. **Snapshot Duration:** Estimated hours –¥–ª—è incremental snapshot
2. **Safety Margin:** 2x factor –¥–ª—è unexpected delays (network issues, connector restarts)
3. **Normal Operations Buffer:** Minimum retention –¥–ª—è non-snapshot periods (–Ω–∞–ø—Ä–∏–º–µ—Ä, 3 –¥–Ω—è)

**Example Calculation:**

```
Snapshot Duration = 27 hours
Safety Margin = 27 √ó 2 = 54 hours
Normal Operations Buffer = 72 hours (3 days)

Total Required Retention = 27 + 54 + 72 = 153 hours ‚âà 7 days
```

**Aurora Maximum Retention:**

```sql
-- Aurora MySQL 3.x supports up to 90 days (2160 hours)
binlog_expire_logs_seconds = 7776000  -- 90 days

-- –ù–æ –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ cases 7 –¥–Ω–µ–π –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
binlog_expire_logs_seconds = 604800  -- 7 days
```

### Monitoring Binlog Health During Snapshot

**CloudWatch Metrics (Aurora):**

```
Metric: ChangeLogBytesUsed
Description: Total size of binlog files
Alert Threshold: > 80% of available storage
```

**Alert Rule:**

```yaml
AlarmName: BinlogStorageHigh
MetricName: ChangeLogBytesUsed
Namespace: AWS/RDS
Threshold: 80000000000  # 80 GB (–µ—Å–ª–∏ total storage 100 GB)
ComparisonOperator: GreaterThanThreshold
EvaluationPeriods: 2
Period: 300  # 5 minutes
```

**Manual Check:**

```sql
-- Total binlog size
SHOW BINARY LOGS;

-- Sum file_size column manually –∏–ª–∏:
SELECT
  ROUND(SUM(file_size) / 1024 / 1024 / 1024, 2) AS total_gb
FROM (
  SELECT file_size FROM information_schema.binlog_files
) AS binlog_summary;
```

**If binlog storage approaching limit:**

1. **Pause incremental snapshot** (stop-snapshot signal)
2. **Increase retention** (–µ—Å–ª–∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç storage)
3. **Resize Aurora storage** (–µ—Å–ª–∏ retention –Ω–µ–ª—å–∑—è —É–≤–µ–ª–∏—á–∏—Ç—å)
4. **Resume snapshot** –ø–æ—Å–ª–µ stabilization

## Hands-On Exercises

### Exercise 1: Setup Signal Table –∏ Trigger Basic Snapshot

**Goal:** –°–æ–∑–¥–∞—Ç—å signal table, –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å connector, trigger incremental snapshot –¥–ª—è –æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã.

**Steps:**

1. –°–æ–∑–¥–∞—Ç—å signal table –≤ MySQL:
   ```sql
   CREATE TABLE inventory.debezium_signal (
     id VARCHAR(36) NOT NULL,
     type VARCHAR(32) NOT NULL,
     data TEXT,
     PRIMARY KEY (id)
   ) ENGINE=InnoDB;

   GRANT INSERT, UPDATE, DELETE ON inventory.debezium_signal TO 'debezium'@'%';
   FLUSH PRIVILEGES;
   ```

2. –û–±–Ω–æ–≤–∏—Ç—å connector config:
   ```json
   "signal.data.collection": "inventory.debezium_signal",
   "incremental.snapshot.chunk.size": "2048"
   ```

3. Redeploy connector:
   ```bash
   curl -X PUT http://localhost:8083/connectors/mysql-inventory-connector/config \
     -H "Content-Type: application/json" \
     -d @updated-connector-config.json
   ```

4. Trigger snapshot:
   ```sql
   INSERT INTO inventory.debezium_signal (id, type, data)
   VALUES (UUID(), 'execute-snapshot', '{"data-collections": ["inventory.products"]}');
   ```

5. Monitor logs:
   ```bash
   docker compose logs -f debezium-connect | grep -i "incremental"
   ```

**Expected outcome:**
- Connector logs show "Incremental snapshot requested"
- Kafka topic `mysql-server.inventory.products` receives snapshot events
- After completion: "Incremental snapshot completed"

### Exercise 2: Monitor Snapshot Progress

**Goal:** Track incremental snapshot —á–µ—Ä–µ–∑ connector logs –∏ estimate completion time.

**Steps:**

1. Trigger snapshot –¥–ª—è large table:
   ```sql
   INSERT INTO inventory.debezium_signal (id, type, data)
   VALUES (UUID(), 'execute-snapshot', '{"data-collections": ["inventory.orders"]}');
   ```

2. Extract chunk timing:
   ```bash
   docker compose logs debezium-connect | grep "Snapshot window" | tail -20
   ```

3. Calculate average chunk time:
   ```python
   # Measure time between consecutive chunks from logs
   # Example: Chunk 1 at 10:00:00, Chunk 2 at 10:00:02 ‚Üí 2 seconds
   avg_chunk_time = 2
   ```

4. Estimate remaining time:
   ```python
   total_rows = 500000  # From SHOW TABLE STATUS
   chunk_size = 2048
   current_chunk = 50

   remaining_chunks = (total_rows / chunk_size) - current_chunk
   estimated_sec = remaining_chunks * avg_chunk_time
   estimated_hours = estimated_sec / 3600

   print(f"Estimated completion: {estimated_hours:.2f} hours")
   ```

### Exercise 3: Stop In-Progress Snapshot

**Goal:** Cancel incremental snapshot –∏ verify clean stop.

**Steps:**

1. Start long-running snapshot:
   ```sql
   INSERT INTO inventory.debezium_signal (id, type, data)
   VALUES (UUID(), 'execute-snapshot', '{"data-collections": ["inventory.orders"]}');
   ```

2. After 1-2 minutes, send stop signal:
   ```sql
   INSERT INTO inventory.debezium_signal (id, type, data)
   VALUES (UUID(), 'stop-snapshot', '{"data-collections": ["inventory.orders"], "type": "INCREMENTAL"}');
   ```

3. Verify stop:
   ```bash
   docker compose logs debezium-connect | grep -E "(stop|abort|cancel)"
   ```

**Expected outcome:**
- Connector logs show "Incremental snapshot stopped"
- Current chunk completes before stopping
- Connector resumes normal binlog streaming

### Exercise 4 (Advanced): Configure Kafka Signal Channel –¥–ª—è Read-Only Scenario

**Goal:** Setup Kafka-based signaling for Aurora read replica CDC.

**Steps:**

1. Create Kafka signal topic:
   ```bash
   docker compose exec kafka kafka-topics \
     --bootstrap-server localhost:9092 \
     --create \
     --topic debezium-signal-mysql \
     --partitions 1 \
     --replication-factor 1
   ```

2. Update connector config:
   ```json
   "signal.kafka.topic": "debezium-signal-mysql",
   "signal.kafka.bootstrap.servers": "kafka:9092",
   "signal.kafka.groupId": "debezium-signal-consumer"
   ```

3. Trigger snapshot via Kafka:
   ```bash
   docker compose exec kafka kafka-console-producer \
     --bootstrap-server localhost:9092 \
     --topic debezium-signal-mysql \
     --property "parse.key=true" \
     --property "key.separator=:"

   # Input:
   {"id":"test-001"}:{"type":"execute-snapshot","data":{"data-collections":["inventory.products"]}}
   ```

4. Verify signal consumption:
   ```bash
   docker compose logs debezium-connect | grep "Incremental snapshot requested"
   ```

## Key Takeaways

**1. Incremental snapshot —Ä–µ—à–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è initial snapshot**
- On-demand snapshot –±–µ–∑ connector restart
- Chunk-based reading (resumable, parallel —Å binlog streaming)
- –ú–æ–∂–Ω–æ snapshot specific tables (–Ω–µ all-or-nothing)

**2. Signal table —Ç—Ä–µ–±—É–µ—Ç —Å—Ç—Ä–æ–≥—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É**
- PRIMARY KEY –Ω–∞ `id` –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω (silent failure –±–µ–∑ PK)
- Schema –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ `database.include.list`
- GRANT INSERT –ø—Ä–∞–≤–∞ –¥–ª—è debezium user

**3. Connector configuration: signal.data.collection**
- Fully-qualified name: `{database}.{table}`
- `incremental.snapshot.chunk.size` –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2048 (tune –¥–ª—è performance)
- Signal table schema –î–û–õ–ñ–ù–ê –±—ã—Ç—å –≤ database whitelist

**4. Triggering snapshot —á–µ—Ä–µ–∑ SQL INSERT**
- Basic: `{"data-collections": ["table"]}`
- Multiple tables: JSON array
- Filtered: `additional-conditions` —Å SQL WHERE clause
- Stop: `type: "stop-snapshot"`

**5. Read-only snapshots —á–µ—Ä–µ–∑ Kafka signal channel**
- –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ signal table –¥–ª—è Aurora read replicas
- –¢—Ä–µ–±—É–µ—Ç GTID prerequisites: `replica_preserve_commit_order=ON`
- Signals sent via `kafka-console-producer`
- `signal.kafka.topic` –≤–º–µ—Å—Ç–æ `signal.data.collection`

**6. Monitoring —á–µ—Ä–µ–∑ JMX metrics –∏ logs**
- `SnapshotRunning`, `RemainingTableCount` (JMX)
- Connector logs: chunk windows, scanned rows
- Estimate completion time: `(total_rows / chunk_size) √ó avg_chunk_time`

**7. Common pitfalls prevention**
- Signal table schema –≤ `database.include.list` (–∏–Ω–∞—á–µ silent failure)
- PRIMARY KEY –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω (no PK = no signals processed)
- Binlog retention >= snapshot duration + safety margin
- Read-only environments: GTID prerequisites critical

**8. Binlog retention planning –¥–ª—è large snapshots**
- Formula: `Snapshot Duration + 2√ó Safety Margin + 3-day buffer`
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: **7 days minimum** (604800 seconds)
- Monitor `ChangeLogBytesUsed` (Aurora CloudWatch)

## –ß—Ç–æ –¥–∞–ª—å—à–µ?

–í —Å–ª–µ–¥—É—é—â–µ–º —É—Ä–æ–∫–µ **"MySQL/Aurora Production Troubleshooting"** –º—ã –∏–∑—É—á–∏–º:
- Debugging failed snapshots (initial –∏ incremental)
- Recovery strategies –ø–æ—Å–ª–µ binlog purge
- Schema evolution –≤–æ –≤—Ä–µ–º—è snapshot
- Performance tuning –¥–ª—è large-table snapshots
- Multi-region CDC challenges (Aurora Global Database)

**–ú–æ–¥—É–ª—å 8 roadmap:**
- ‚úÖ Lesson 1-9: MySQL binlog, GTID, Aurora parameter groups, snapshot modes
- ‚úÖ **Lesson 12: Incremental snapshots** ‚Üê –í—ã –∑–¥–µ—Å—å
- üîú Lesson 13: Production troubleshooting
- üîú Lesson 14: MySQL/Aurora decision matrix

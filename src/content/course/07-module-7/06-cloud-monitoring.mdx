---
title: "End-to-End ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ CDC Pipeline"
description: "Cloud Monitoring Ğ´Ğ»Ñ Ğ²ÑĞµÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²: Cloud SQL, Debezium, Pub/Sub, Dataflow, Cloud Run"
order: 6
difficulty: "advanced"
estimatedTime: 45
topics: ["Cloud Monitoring", "Alerting", "Metrics", "Dashboards", "SRE"]
prerequisites: ["module-6/01-cloud-sql-setup", "module-6/02-debezium-server-pubsub", "module-6/04-dataflow-bigquery", "module-6/05-cloud-run-event-driven"]
---

import { Mermaid } from '../../../components/Mermaid.tsx';

# End-to-End ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ CDC Pipeline

Ğ’ Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ñ… ÑƒÑ€Ğ¾ĞºĞ°Ñ… Ğ¼Ñ‹ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ†ĞµĞ½Ğ½Ñ‹Ğ¹ CDC pipeline Ğ½Ğ° GCP: Cloud SQL â†’ Debezium Server â†’ Pub/Sub â†’ Dataflow/Cloud Run. Ğ¢ĞµĞ¿ĞµÑ€ÑŒ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ **observability** Ğ´Ğ»Ñ Ğ²ÑĞµĞ³Ğ¾ pipeline.

## Ğ—Ğ°Ñ‡ĞµĞ¼ Ğ½ÑƒĞ¶ĞµĞ½ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ CDC pipeline?

CDC pipeline â€” ÑÑ‚Ğ¾ **ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°** Ğ´Ğ»Ñ ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. Ğ›ÑĞ±Ğ¾Ğ¹ ÑĞ±Ğ¾Ğ¹ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº:

- **Data loss** (Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğ¸ WAL)
- **Data inconsistency** (BigQuery replica Ğ¾Ñ‚ÑÑ‚Ğ°ĞµÑ‚ Ğ¾Ñ‚ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°)
- **SLA Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ñ** (Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ° Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ > Ğ´Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ñ€Ğ¾Ğ³Ğ°)

**ĞŸÑ€Ğ¾Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³** Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶Ğ¸Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ´Ğ¾ Ğ¸Ñ… Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ Ğ½Ğ° Ğ±Ğ¸Ğ·Ğ½ĞµÑ.

### ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°

<Mermaid chart={`
graph TD
    subgraph "Source"
        CS[Cloud SQL<br/>PostgreSQL]
    end

    subgraph "CDC Engine"
        DBS[Debezium<br/>Server]
    end

    subgraph "Messaging"
        PS[Pub/Sub<br/>Topics]
    end

    subgraph "Consumers"
        DF[Dataflow<br/>BigQuery]
        CR[Cloud Run<br/>Processors]
    end

    subgraph "Monitoring Points"
        M1[CPU/Disk/WAL<br/>Replication Slots]
        M2[Lag/Throughput<br/>Queue Capacity]
        M3[Backlog/DLQ<br/>Publish Rate]
        M4[System Lag<br/>vCPU Usage]
        M5[Request Count<br/>Error Rate]
    end

    CS -.->|Monitor| M1
    DBS -.->|Monitor| M2
    PS -.->|Monitor| M3
    DF -.->|Monitor| M4
    CR -.->|Monitor| M5

    style M1 fill:#ff6b6b
    style M2 fill:#ff6b6b
    style M3 fill:#ff6b6b
    style M4 fill:#ff6b6b
    style M5 fill:#ff6b6b
`} client:visible />

Ğ’ ÑÑ‚Ğ¾Ğ¼ ÑƒÑ€Ğ¾ĞºĞµ Ğ¼Ñ‹ Ñ€Ğ°Ğ·Ğ±ĞµÑ€ĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, alerts, dashboards Ğ¸ runbooks Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ°.

### Ğ¡Ğ²ÑĞ·ÑŒ Ñ Module 3

Ğ’ [Module 3 Production Operations](/course/04-module-4) Ğ¼Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ»Ğ¸ Ñ Prometheus Ğ¸ Grafana Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° Debezium. Ğ—Ğ´ĞµÑÑŒ Ğ¼Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ Ñ‚Ğµ Ğ¶Ğµ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ğ¸, Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ **Cloud Monitoring** â€” ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼Ñ‹Ğ¹ ÑĞµÑ€Ğ²Ğ¸Ñ GCP Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ²ÑĞµÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ².

---

## Cloud SQL PostgreSQL Monitoring

### Built-in Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸

Cloud SQL Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ² Cloud Monitoring:

| ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ° | Ğ¢Ğ¸Ğ¿ | ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ |
|---------|-----|----------|
| `cloudsql.googleapis.com/database/cpu/utilization` | Gauge | CPU ÑƒÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (0.0-1.0) |
| `cloudsql.googleapis.com/database/disk/utilization` | Gauge | Disk ÑƒÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (0.0-1.0) |
| `cloudsql.googleapis.com/database/disk/bytes_used` | Gauge | Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ disk Ğ² Ğ±Ğ°Ğ¹Ñ‚Ğ°Ñ… |
| `cloudsql.googleapis.com/database/postgresql/num_backends` | Gauge | ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğ¹ |
| `cloudsql.googleapis.com/database/memory/utilization` | Gauge | Memory ÑƒÑ‚Ğ¸Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ |

### Replication Slot Monitoring (Custom Query)

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°:** Cloud SQL Ğ½Ğµ ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ replication slots Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ.

**Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ:** Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Cloud SQL Insights Ğ´Ğ»Ñ custom query Ğ¸Ğ»Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Cloud Function Ğ´Ğ»Ñ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°.

#### Monitoring Query

```sql
-- Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑÑ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ 60 ÑĞµĞºÑƒĞ½Ğ´ Ñ‡ĞµÑ€ĞµĞ· Cloud Function Ğ¸Ğ»Ğ¸ Cloud Scheduler
SELECT
    slot_name,
    slot_type,
    plugin,
    active,
    restart_lsn,
    confirmed_flush_lsn,
    -- Ğ‘Ğ°Ğ¹Ñ‚Ñ‹ Ğ¾Ñ‚ÑÑ‚Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ñ‚ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ³Ğ¾ WAL
    pg_wal_lsn_diff(pg_current_wal_lsn(), confirmed_flush_lsn) AS lag_bytes,
    -- Ğ§ĞµĞ»Ğ¾Ğ²ĞµĞºĞ¾-Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚
    pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(), confirmed_flush_lsn)) AS lag_size
FROM pg_replication_slots
WHERE slot_name = 'debezium_slot';
```

#### Cloud Function Ğ´Ğ»Ñ ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸

```python
from google.cloud import monitoring_v3
import pg8000
import time

def export_replication_lag(request):
    """Cloud Function Ğ´Ğ»Ñ ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ° replication slot lag Ğ² Cloud Monitoring."""

    # ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº Cloud SQL Ñ‡ĞµÑ€ĞµĞ· Unix socket (Cloud SQL Proxy)
    conn = pg8000.connect(
        database="production",
        user="monitoring_user",
        password="...",
        host="/cloudsql/PROJECT:REGION:INSTANCE"
    )

    cursor = conn.cursor()
    cursor.execute("""
        SELECT pg_wal_lsn_diff(pg_current_wal_lsn(), confirmed_flush_lsn) AS lag_bytes
        FROM pg_replication_slots
        WHERE slot_name = 'debezium_slot' AND active = true;
    """)

    result = cursor.fetchone()
    lag_bytes = result[0] if result else 0

    # Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚ Ğ² Cloud Monitoring ĞºĞ°Ğº custom metric
    client = monitoring_v3.MetricServiceClient()
    project_name = f"projects/YOUR_PROJECT_ID"

    series = monitoring_v3.TimeSeries()
    series.metric.type = "custom.googleapis.com/cloudsql/replication_lag_bytes"
    series.resource.type = "global"

    now = time.time()
    seconds = int(now)
    nanos = int((now - seconds) * 10**9)
    interval = monitoring_v3.TimeInterval(
        {"end_time": {"seconds": seconds, "nanos": nanos}}
    )
    point = monitoring_v3.Point(
        {"interval": interval, "value": {"int64_value": lag_bytes}}
    )
    series.points = [point]

    client.create_time_series(name=project_name, time_series=[series])

    return f"Exported lag_bytes: {lag_bytes}"
```

### Alert: WAL Disk Bloat

```yaml
# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· gcloud Ğ¸Ğ»Ğ¸ Console
displayName: "Cloud SQL - Disk Utilization High (WAL Bloat)"
conditions:
  - displayName: "Disk > 80%"
    conditionThreshold:
      filter: |
        resource.type="cloudsql_database"
        metric.type="cloudsql.googleapis.com/database/disk/utilization"
      comparison: COMPARISON_GT
      thresholdValue: 0.8
      duration: 300s
      aggregations:
        - alignmentPeriod: 60s
          perSeriesAligner: ALIGN_MEAN

notificationChannels:
  - projects/YOUR_PROJECT/notificationChannels/email-oncall

documentation:
  content: |
    Cloud SQL disk utilization exceeded 80%.

    **Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°:** WAL bloat Ğ¸Ğ·-Ğ·Ğ° Ğ½ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ replication slot.

    **Runbook:**
    1. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ pg_replication_slots: `SELECT * FROM pg_replication_slots;`
    2. Ğ•ÑĞ»Ğ¸ slot Ğ½ĞµĞ°ĞºÑ‚Ğ¸Ğ²ĞµĞ½ Ğ¸ lag_bytes > 10GB â†’ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Debezium Server logs
    3. Ğ•ÑĞ»Ğ¸ Debezium Server ÑƒĞ¿Ğ°Ğ» â†’ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ pod
    4. Ğ•ÑĞ»Ğ¸ slot orphaned â†’ ÑƒĞ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ: `SELECT pg_drop_replication_slot('slot_name');`
```

---

## Debezium Server Metrics

Debezium Server ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ JMX Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Prometheus JMX exporter.

### ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸

| ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ° (JMX) | Cloud Monitoring Type | ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ |
|---------------|----------------------|----------|
| `MilliSecondsBehindSource` | Gauge | Ğ—Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ° Ñ€ĞµĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ² Ğ¼Ğ¸Ğ»Ğ»Ğ¸ÑĞµĞºÑƒĞ½Ğ´Ğ°Ñ… |
| `QueueRemainingCapacity` | Gauge | Ğ¡Ğ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾ Ğ² internal queue |
| `TotalNumberOfEventsSeen` | Counter | Ğ’ÑĞµĞ³Ğ¾ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾ |
| `NumberOfEventsFiltered` | Counter | Ğ¡Ğ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ Ğ¾Ñ‚Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ (Ğ½Ğµ Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ) |
| `SnapshotRunning` | Gauge | 1 ĞµÑĞ»Ğ¸ snapshot Ğ¸Ğ´ĞµÑ‚, 0 ĞµÑĞ»Ğ¸ Ğ½ĞµÑ‚ |

### GKE Integration: Google Cloud Managed Service for Prometheus

```yaml
# PodMonitoring CRD Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ñ Debezium Server
apiVersion: monitoring.googleapis.com/v1
kind: PodMonitoring
metadata:
  name: debezium-server
  namespace: cdc
spec:
  selector:
    matchLabels:
      app: debezium-server
  endpoints:
  - port: metrics  # JMX exporter Ğ¿Ğ¾Ñ€Ñ‚ (Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ 8080)
    interval: 30s
    path: /q/metrics  # Quarkus metrics endpoint
```

### Custom Metric Export Ğ² Cloud Monitoring

Debezium Server Quarkus ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Prometheus. GKE Ñ Managed Service for Prometheus Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ Ğ¸Ñ… Ğ² Cloud Monitoring Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚:

```
# Prometheus format (JMX exporter)
debezium_metrics_MilliSecondsBehindSource{...} 5432

# Cloud Monitoring format (auto-converted)
metric.type="prometheus.googleapis.com/debezium_metrics_MilliSecondsBehindSource/gauge"
```

### Alert: High Replication Lag

```yaml
displayName: "Debezium - Replication Lag High"
conditions:
  - displayName: "Lag > 60 seconds"
    conditionThreshold:
      filter: |
        resource.type="k8s_pod"
        resource.labels.namespace_name="cdc"
        metric.type="prometheus.googleapis.com/debezium_metrics_MilliSecondsBehindSource/gauge"
      comparison: COMPARISON_GT
      thresholdValue: 60000  # 60 seconds in milliseconds
      duration: 300s  # 5 Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ğ¿Ğ¾Ğ´Ñ€ÑĞ´
      aggregations:
        - alignmentPeriod: 60s
          perSeriesAligner: ALIGN_MEAN

notificationChannels:
  - projects/YOUR_PROJECT/notificationChannels/pagerduty-oncall

alertStrategy:
  autoClose: 1800s  # Auto-resolve ĞµÑĞ»Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ° Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ»Ğ°ÑÑŒ

documentation:
  content: |
    Debezium replication lag exceeded 60 seconds.

    **Impact:** Data Ğ² BigQuery/Pub/Sub Ğ¾Ñ‚ÑÑ‚Ğ°ĞµÑ‚ Ğ¾Ñ‚ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° Ğ½Ğ° 60+ ÑĞµĞºÑƒĞ½Ğ´.

    **Runbook:**
    1. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Cloud SQL CPU/memory utilization
    2. Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ replication slot query: SELECT * FROM pg_replication_slots;
    3. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Debezium Server logs: kubectl logs -n cdc debezium-server-xxx
    4. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Pub/Sub publish throughput
    5. Ğ•ÑĞ»Ğ¸ lag Ğ½Ğµ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ÑÑ â†’ Ñ€Ğ°ÑÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ scale up Cloud SQL tier
```

### Alert: Queue Backpressure

```yaml
displayName: "Debezium - Queue Remaining Capacity Low"
conditions:
  - displayName: "Queue < 20% capacity"
    conditionThreshold:
      filter: |
        resource.type="k8s_pod"
        metric.type="prometheus.googleapis.com/debezium_metrics_QueueRemainingCapacity/gauge"
      comparison: COMPARISON_LT
      thresholdValue: 1638  # 20% of default 8192
      duration: 120s

documentation:
  content: |
    Debezium internal queue has < 20% remaining capacity.

    **ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°:** Pub/Sub sink Ğ½Ğµ ÑƒÑĞ¿ĞµĞ²Ğ°ĞµÑ‚ publish ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ (backpressure).

    **Runbook:**
    1. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Pub/Sub errors Ğ² Debezium logs
    2. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Pub/Sub quota limits
    3. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Pub/Sub topic publish rate metrics
    4. Ğ Ğ°ÑÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ max.queue.size Ğ² ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğµ
```

---

## Pub/Sub Metrics

### Built-in Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸

| ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ° | Ğ¢Ğ¸Ğ¿ | ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ |
|---------|-----|----------|
| `pubsub.googleapis.com/subscription/oldest_unacked_message_age` | Gauge | Ğ’Ğ¾Ğ·Ñ€Ğ°ÑÑ‚ ÑÑ‚Ğ°Ñ€ĞµĞ¹ÑˆĞµĞ³Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ (ÑĞµĞºÑƒĞ½Ğ´Ñ‹) |
| `pubsub.googleapis.com/subscription/num_undelivered_messages` | Gauge | ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ½ĞµĞ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ |
| `pubsub.googleapis.com/topic/send_message_operation_count` | Counter | ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ publish Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ |
| `pubsub.googleapis.com/subscription/pull_request_count` | Counter | ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ pull requests Ğ¾Ñ‚ subscribers |
| `pubsub.googleapis.com/subscription/dead_letter_message_count` | Counter | Ğ¡Ğ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ² Dead Letter Queue |

### Alert: Pub/Sub Backlog Growing

```yaml
displayName: "Pub/Sub - Subscription Backlog High"
conditions:
  - displayName: "Oldest unacked message > 5 minutes"
    conditionThreshold:
      filter: |
        resource.type="pubsub_subscription"
        metric.type="pubsub.googleapis.com/subscription/oldest_unacked_message_age"
      comparison: COMPARISON_GT
      thresholdValue: 300  # 5 minutes
      duration: 300s
      aggregations:
        - alignmentPeriod: 60s
          perSeriesAligner: ALIGN_MAX

documentation:
  content: |
    Pub/Sub subscription backlog exceeds 5 minutes.

    **Impact:** Consumers (Dataflow, Cloud Run) Ğ½Ğµ ÑƒÑĞ¿ĞµĞ²Ğ°ÑÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ.

    **Runbook:**
    1. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Dataflow job status Ğ¸ worker count
    2. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Cloud Run error rate (5xx responses)
    3. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ dead_letter_message_count
    4. Ğ•ÑĞ»Ğ¸ consumer health OK â†’ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ Dataflow workers Ğ¸Ğ»Ğ¸ Cloud Run max-instances
```

### Alert: Dead Letter Queue Messages

```yaml
displayName: "Pub/Sub - Dead Letter Messages Detected"
conditions:
  - displayName: "DLQ message count > 0"
    conditionThreshold:
      filter: |
        resource.type="pubsub_subscription"
        metric.type="pubsub.googleapis.com/subscription/dead_letter_message_count"
      comparison: COMPARISON_GT
      thresholdValue: 0
      duration: 60s

documentation:
  content: |
    Messages detected in Dead Letter Queue.

    **ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°:** Consumer Ğ²ĞµÑ€Ğ½ÑƒĞ» 4xx Ğ¸Ğ»Ğ¸ Ğ¸ÑÑ‡ĞµÑ€Ğ¿Ğ°Ğ» max delivery attempts.

    **Runbook:**
    1. Pull ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸Ğ· DLQ: gcloud pubsub subscriptions pull cdc-dead-letter-sub --limit=10
    2. ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ (Ğ½ĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Debezium event?)
    3. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ consumer logs Ğ½Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ°
    4. Ğ•ÑĞ»Ğ¸ bug Ğ² consumer â†’ fix Ğ¸ redeploy
    5. Ğ•ÑĞ»Ğ¸ bad data â†’ skip Ğ¸Ğ»Ğ¸ manual replay
```

---

## Dataflow Metrics

### Built-in Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸

| ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ° | Ğ¢Ğ¸Ğ¿ | ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ |
|---------|-----|----------|
| `dataflow.googleapis.com/job/system_lag` | Gauge | System lag (ÑĞµĞºÑƒĞ½Ğ´Ñ‹) â€” Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ |
| `dataflow.googleapis.com/job/elements_produced_count` | Counter | Ğ­Ğ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾ |
| `dataflow.googleapis.com/job/current_num_vcpus` | Gauge | ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ vCPU Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ |
| `dataflow.googleapis.com/job/is_failed` | Gauge | 1 ĞµÑĞ»Ğ¸ job failed, 0 ĞµÑĞ»Ğ¸ running |
| `dataflow.googleapis.com/job/current_vCPU_time` | Counter | vCPU time Ğ´Ğ»Ñ cost tracking |

### Alert: Dataflow System Lag

```yaml
displayName: "Dataflow - System Lag High"
conditions:
  - displayName: "System lag > 60 seconds"
    conditionThreshold:
      filter: |
        resource.type="dataflow_job"
        metric.type="dataflow.googleapis.com/job/system_lag"
      comparison: COMPARISON_GT
      thresholdValue: 60
      duration: 300s
      aggregations:
        - alignmentPeriod: 60s
          perSeriesAligner: ALIGN_MAX

documentation:
  content: |
    Dataflow job system lag exceeds 60 seconds.

    **Impact:** BigQuery replica Ğ¾Ñ‚ÑÑ‚Ğ°ĞµÑ‚ Ğ¾Ñ‚ Pub/Sub Ğ½Ğ° 60+ ÑĞµĞºÑƒĞ½Ğ´.

    **Runbook:**
    1. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ current_num_vcpus â€” Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚ Ğ»Ğ¸ max workers?
    2. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ BigQuery streaming insert quota
    3. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Pub/Sub backlog (upstream Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°?)
    4. Ğ•ÑĞ»Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ workers â†’ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ maxNumWorkers Ğ² job Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ñ…
    5. Ğ•ÑĞ»Ğ¸ BigQuery throttling â†’ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ quota
```

### Cost Monitoring

```yaml
# ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Dataflow job
displayName: "Dataflow - Monthly Cost Alert"
conditions:
  - displayName: "vCPU hours > budget"
    conditionThreshold:
      filter: |
        resource.type="dataflow_job"
        metric.type="dataflow.googleapis.com/job/current_vCPU_time"
      comparison: COMPARISON_GT
      thresholdValue: 720000  # 720k ÑĞµĞºÑƒĞ½Ğ´ = 200 vCPU-hours
      duration: 86400s  # Daily check
      aggregations:
        - alignmentPeriod: 3600s
          perSeriesAligner: ALIGN_RATE

documentation:
  content: |
    Dataflow job vCPU usage exceeded budget.

    **Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸:**
    1. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ autoscaling Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹
    2. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ at-least-once mode Ğ²Ğ¼ĞµÑÑ‚Ğ¾ exactly-once (2x cost reduction)
    3. Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ updateFrequencySecs Ğ´Ğ»Ñ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ñ‹ MERGE
```

---

## Cloud Run Metrics

### Built-in Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸

| ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ° | Ğ¢Ğ¸Ğ¿ | ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ |
|---------|-----|----------|
| `run.googleapis.com/request_count` | Counter | ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ HTTP requests |
| `run.googleapis.com/request_latencies` | Distribution | Latency Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ |
| `run.googleapis.com/container/instance_count` | Gauge | ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¾Ğ² |
| `run.googleapis.com/container/billable_instance_time` | Counter | Billable time (Ğ´Ğ»Ñ cost tracking) |
| `run.googleapis.com/request_count` (response_code) | Counter | Requests Ğ¿Ğ¾ HTTP ĞºĞ¾Ğ´Ñƒ |

### Alert: Cloud Run Error Rate

```yaml
displayName: "Cloud Run - High Error Rate"
conditions:
  - displayName: "5xx error rate > 5%"
    conditionThreshold:
      filter: |
        resource.type="cloud_run_revision"
        metric.type="run.googleapis.com/request_count"
        metric.labels.response_code_class="5xx"
      comparison: COMPARISON_GT
      thresholdValue: 0.05  # 5% of requests
      duration: 180s
      aggregations:
        - alignmentPeriod: 60s
          perSeriesAligner: ALIGN_RATE
        - crossSeriesReducer: REDUCE_SUM
          groupByFields: ["resource.service_name"]

documentation:
  content: |
    Cloud Run service returning > 5% 5xx errors.

    **Impact:** CDC events Ğ½Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ÑÑ‚ÑÑ, Pub/Sub Ğ±ÑƒĞ´ĞµÑ‚ retry.

    **Runbook:**
    1. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Cloud Run logs: gcloud logging read --limit 50 --format json
    2. ĞŸĞ¾Ğ¸ÑĞº exceptions Ğ² application logs
    3. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ dependency services (Elasticsearch, Redis, Slack API)
    4. Ğ•ÑĞ»Ğ¸ external service down â†’ implement circuit breaker
    5. Ğ•ÑĞ»Ğ¸ code bug â†’ rollback to previous revision
```

### Alert: Cloud Run High Latency

```yaml
displayName: "Cloud Run - Request Latency High"
conditions:
  - displayName: "P99 latency > 5 seconds"
    conditionThreshold:
      filter: |
        resource.type="cloud_run_revision"
        metric.type="run.googleapis.com/request_latencies"
      comparison: COMPARISON_GT
      thresholdValue: 5000  # 5 seconds in milliseconds
      duration: 300s
      aggregations:
        - alignmentPeriod: 60s
          perSeriesAligner: ALIGN_DELTA
        - crossSeriesReducer: REDUCE_PERCENTILE_99

documentation:
  content: |
    Cloud Run P99 latency exceeds 5 seconds.

    **ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°:** ĞœĞµĞ´Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹ (Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ API, DB queries).

    **Runbook:**
    1. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Cloud Run logs Ğ´Ğ»Ñ slow requests
    2. ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ external API latency (Elasticsearch, Redis)
    3. Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ñ‚Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚Ñ‹ Ğ½Ğ° Ğ²Ñ‹Ğ·Ğ¾Ğ²Ñ‹ external services
    4. Ğ Ğ°ÑÑĞ¼Ğ¾Ñ‚Ñ€ĞµÑ‚ÑŒ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ CPU/memory Ğ´Ğ»Ñ Cloud Run
```

---

## Unified Dashboard

### Dashboard Structure

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         CDC Pipeline Health Dashboard                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Row 1: High-Level Health                                    â•‘
â•‘  [Cloud SQL Status] [Debezium Status] [Pub/Sub Status]     â•‘
â•‘  [Dataflow Status]  [Cloud Run Status]                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Row 2: Source Metrics (Cloud SQL)                          â•‘
â•‘  [CPU Utilization] [Disk Utilization] [Active Connections] â•‘
â•‘  [WAL Size] [Replication Slot Lag]                         â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Row 3: CDC Engine (Debezium)                               â•‘
â•‘  [MilliSecondsBehindSource] [Events Processed/sec]         â•‘
â•‘  [Queue Remaining Capacity]                                 â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Row 4: Messaging (Pub/Sub)                                 â•‘
â•‘  [Oldest Unacked Message Age] [Num Undelivered Messages]   â•‘
â•‘  [Dead Letter Queue Count]                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Row 5: Consumers (Dataflow + Cloud Run)                    â•‘
â•‘  [Dataflow System Lag] [Dataflow vCPUs]                    â•‘
â•‘  [Cloud Run Request Rate] [Cloud Run Error Rate]           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

<Mermaid chart={`
graph TD
    subgraph "Dashboard Layout"
        subgraph "Row 1: Health Indicators"
            H1[ğŸŸ¢ Cloud SQL<br/>Healthy]
            H2[ğŸŸ¢ Debezium<br/>Healthy]
            H3[ğŸŸ¡ Pub/Sub<br/>Warning]
            H4[ğŸŸ¢ Dataflow<br/>Healthy]
            H5[ğŸ”´ Cloud Run<br/>Critical]
        end

        subgraph "Row 2: Source"
            S1[CPU: 45%]
            S2[Disk: 62%]
            S3[WAL Lag: 2.5GB]
        end

        subgraph "Row 3: CDC"
            C1[Lag: 12s]
            C2[Throughput: 350 evt/s]
            C3[Queue: 75% free]
        end

        subgraph "Row 4: Messaging"
            M1[Oldest Msg: 45s]
            M2[Backlog: 1250]
            M3[DLQ: 0]
        end

        subgraph "Row 5: Consumers"
            D1[System Lag: 8s]
            D2[Workers: 5]
            R1[Req/s: 120]
            R2[Error Rate: 12%]
        end
    end

    style H5 fill:#ff6b6b
    style H3 fill:#ffd93d
    style R2 fill:#ff6b6b
`} client:visible />

### JSON Dashboard Definition (Excerpt)

```json
{
  "displayName": "CDC Pipeline - End to End",
  "mosaicLayout": {
    "columns": 12,
    "tiles": [
      {
        "width": 4,
        "height": 4,
        "widget": {
          "title": "Debezium Replication Lag",
          "xyChart": {
            "dataSets": [{
              "timeSeriesQuery": {
                "timeSeriesFilter": {
                  "filter": "metric.type=\"prometheus.googleapis.com/debezium_metrics_MilliSecondsBehindSource/gauge\" resource.type=\"k8s_pod\"",
                  "aggregation": {
                    "alignmentPeriod": "60s",
                    "perSeriesAligner": "ALIGN_MEAN"
                  }
                }
              },
              "plotType": "LINE",
              "targetAxis": "Y1"
            }],
            "thresholds": [
              {
                "value": 30000,
                "color": "YELLOW",
                "label": "Warning (30s)"
              },
              {
                "value": 60000,
                "color": "RED",
                "label": "Critical (60s)"
              }
            ],
            "yAxis": {
              "label": "Milliseconds",
              "scale": "LINEAR"
            }
          }
        }
      },
      {
        "width": 4,
        "height": 4,
        "widget": {
          "title": "Pub/Sub Subscription Backlog",
          "xyChart": {
            "dataSets": [{
              "timeSeriesQuery": {
                "timeSeriesFilter": {
                  "filter": "metric.type=\"pubsub.googleapis.com/subscription/num_undelivered_messages\" resource.type=\"pubsub_subscription\"",
                  "aggregation": {
                    "alignmentPeriod": "60s",
                    "perSeriesAligner": "ALIGN_MEAN",
                    "crossSeriesReducer": "REDUCE_SUM",
                    "groupByFields": ["resource.subscription_id"]
                  }
                }
              }
            }]
          }
        }
      }
    ]
  }
}
```

**Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ dashboard Ñ‡ĞµÑ€ĞµĞ· gcloud:**

```bash
# Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚ dashboard Ğ² JSON
gcloud monitoring dashboards create --config-from-file=cdc-dashboard.json

# ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ³Ğ¾ dashboard
gcloud monitoring dashboards update DASHBOARD_ID --config-from-file=cdc-dashboard.json
```

---

## Alerting Policies

### Alert Hierarchy

| Severity | Notification Channel | Response Time |
|----------|---------------------|---------------|
| **CRITICAL** | PagerDuty (on-call) | Immediate (page) |
| **WARNING** | Email, Slack | Within 1 hour |
| **INFO** | Slack only | Next business day |

### Example: Complete Alert Policy YAML

```yaml
# alert-debezium-lag.yaml
displayName: "CDC Pipeline - Debezium Lag High"
combiner: OR
conditions:
  - displayName: "Lag > 60 seconds (CRITICAL)"
    conditionThreshold:
      filter: |
        resource.type="k8s_pod"
        resource.labels.namespace_name="cdc"
        metric.type="prometheus.googleapis.com/debezium_metrics_MilliSecondsBehindSource/gauge"
      comparison: COMPARISON_GT
      thresholdValue: 60000
      duration: 300s
      aggregations:
        - alignmentPeriod: 60s
          perSeriesAligner: ALIGN_MEAN

notificationChannels:
  - projects/YOUR_PROJECT/notificationChannels/pagerduty-oncall
  - projects/YOUR_PROJECT/notificationChannels/slack-cdc-alerts

alertStrategy:
  autoClose: 1800s  # Auto-resolve after 30 min if metric normalizes
  notificationRateLimit:
    period: 3600s  # Max 1 notification per hour (prevent alert storm)

documentation:
  content: |
    # Debezium Replication Lag Exceeded 60 Seconds

    ## Impact
    - Data Ğ² BigQuery Ğ¾Ñ‚ÑÑ‚Ğ°ĞµÑ‚ Ğ¾Ñ‚ Cloud SQL Ğ½Ğ° 60+ ÑĞµĞºÑƒĞ½Ğ´
    - Potential WAL bloat if lag continues growing

    ## Runbook

    ### 1. Check Cloud SQL Health
    ```bash
    gcloud sql instances describe INSTANCE_NAME
    ```
    - CPU utilization > 80%? â†’ Scale up instance tier
    - Disk utilization > 80%? â†’ Check replication slot lag

    ### 2. Query Replication Slot
    ```sql
    SELECT slot_name, active,
      pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(), confirmed_flush_lsn)) AS lag
    FROM pg_replication_slots;
    ```
    - Slot inactive? â†’ Check Debezium Server logs
    - Lag growing continuously? â†’ Pub/Sub publish backpressure

    ### 3. Check Debezium Server Logs
    ```bash
    kubectl logs -n cdc deployment/debezium-server --tail=100
    ```
    - Look for: Pub/Sub publish errors, connection timeouts, OOM

    ### 4. Check Pub/Sub Publish Throughput
    - Console â†’ Pub/Sub â†’ Topics â†’ cdc.public.* â†’ Metrics
    - Publish rate dropping? â†’ Debezium Server issue
    - Publish rate high but subscription backlog growing? â†’ Consumer issue

    ### 5. Mitigation Actions
    - **Short-term:** Restart Debezium Server pod
    - **Medium-term:** Scale Cloud SQL instance tier
    - **Long-term:** Optimize table filtering, reduce max.batch.size

    ## Escalation
    If lag > 300 seconds (5 minutes): Page senior SRE
```

**Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ alert policy:**

```bash
gcloud alpha monitoring policies create --policy-from-file=alert-debezium-lag.yaml
```

---

## Runbooks

### Runbook 1: High Replication Lag

**Trigger:** Debezium `MilliSecondsBehindSource` > 60 seconds

**Steps:**

1. **Check Cloud SQL CPU/Memory**
   ```bash
   gcloud sql instances describe INSTANCE_NAME --format="value(settings.tier, state)"
   ```
   - If CPU > 80% â†’ Scale up instance tier

2. **Query `pg_replication_slots`**
   ```sql
   SELECT * FROM pg_replication_slots WHERE slot_name = 'debezium_slot';
   ```
   - Check `active` field
   - Check `confirmed_flush_lsn` lag

3. **Check Debezium Server Logs**
   ```bash
   kubectl logs -n cdc deployment/debezium-server --tail=200
   ```
   - Search for: "Failed to publish", "Connection refused", "OutOfMemoryError"

4. **Verify Pub/Sub Publish Throughput**
   - Cloud Console â†’ Pub/Sub â†’ Topics â†’ cdc.public.orders
   - Check "Publish message operations" metric

5. **Mitigation:**
   - Restart Debezium Server: `kubectl rollout restart -n cdc deployment/debezium-server`
   - If persistent â†’ Scale Cloud SQL tier or reduce `max.batch.size`

---

### Runbook 2: Pub/Sub Backlog Growing

**Trigger:** `oldest_unacked_message_age` > 300 seconds

**Steps:**

1. **Check Cloud Run Error Rate**
   ```bash
   gcloud monitoring time-series list \
     --filter='metric.type="run.googleapis.com/request_count" AND metric.label.response_code_class="5xx"' \
     --interval-start-time="2024-01-01T00:00:00Z"
   ```
   - If error rate > 5% â†’ Check Cloud Run logs

2. **Check Dataflow Worker Health**
   ```bash
   gcloud dataflow jobs describe JOB_ID --region=us-central1
   ```
   - Check `currentState`: RUNNING, FAILED, CANCELLED
   - Check `currentWorkerCount` vs `maxWorkerCount`

3. **Review Dead Letter Queue**
   ```bash
   gcloud pubsub subscriptions pull cdc-dead-letter-sub --limit=10 --auto-ack
   ```
   - Analyze message structure for parsing errors

4. **Mitigation:**
   - **Cloud Run:** Increase `--max-instances`
   - **Dataflow:** Increase `maxNumWorkers` in job parameters
   - **DLQ messages:** Fix consumer code bug and redeploy

---

### Runbook 3: Cloud SQL Disk Full

**Trigger:** `disk/utilization` > 90%

**Steps:**

1. **Check WAL Size**
   ```sql
   SELECT pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(), '0/0')) AS current_wal_size;
   ```

2. **Check Inactive Replication Slots**
   ```sql
   SELECT slot_name, active, pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn)) AS lag
   FROM pg_replication_slots
   WHERE active = false;
   ```

3. **Drop Orphaned Slots**
   ```sql
   SELECT pg_drop_replication_slot('orphaned_slot_name');
   ```

4. **Emergency Disk Expansion**
   ```bash
   gcloud sql instances patch INSTANCE_NAME --storage-size=200GB
   ```

---

## Cost Optimization

### 1. Metrics Sampling

**Trade-off:** ĞœĞµĞ½ĞµĞµ Ñ‡Ğ°ÑÑ‚Ñ‹Ğ¹ ÑĞ±Ğ¾Ñ€ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº â†’ Ğ½Ğ¸Ğ¶Ğµ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ, Ğ½Ğ¾ Ğ²Ñ‹ÑˆĞµ latency Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼.

```yaml
# PodMonitoring Ğ´Ğ»Ñ Debezium
spec:
  endpoints:
  - port: metrics
    interval: 60s  # 60s Ğ²Ğ¼ĞµÑÑ‚Ğ¾ 30s = 50% cost reduction
```

### 2. Log Exclusion Filters

Debezium Server Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ verbose Ğ»Ğ¾Ğ³Ğ¸. Ğ˜ÑĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ğ½ĞµĞ²Ğ°Ğ¶Ğ½Ñ‹Ğµ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸:

```bash
gcloud logging sinks create exclude-debug-logs \
  logging.googleapis.com/projects/YOUR_PROJECT/logs \
  --log-filter='resource.type="k8s_pod" AND severity<"INFO"'
```

### 3. Alert Aggregation

Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ `notificationRateLimit` Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ alert storms:

```yaml
alertStrategy:
  notificationRateLimit:
    period: 3600s  # Max 1 notification per hour
```

---

## Monitoring Points Diagram

<Mermaid chart={`
graph LR
    subgraph "Source"
        CS[Cloud SQL] -->|Monitor| M1[fa:fa-chart-line<br/>CPU/Disk<br/>Connections<br/>WAL]
    end

    subgraph "CDC Engine"
        DBS[Debezium<br/>Server] -->|Monitor| M2[fa:fa-chart-line<br/>Lag<br/>Throughput<br/>Queue]
    end

    subgraph "Messaging"
        PS[Pub/Sub] -->|Monitor| M3[fa:fa-chart-line<br/>Backlog<br/>DLQ<br/>Publish Rate]
    end

    subgraph "Consumers"
        DF[Dataflow] -->|Monitor| M4[fa:fa-chart-line<br/>System Lag<br/>Workers<br/>Cost]
        CR[Cloud Run] -->|Monitor| M5[fa:fa-chart-line<br/>Requests<br/>Errors<br/>Latency]
    end

    M1 --> CM[Cloud<br/>Monitoring]
    M2 --> CM
    M3 --> CM
    M4 --> CM
    M5 --> CM

    CM --> DASH[Unified<br/>Dashboard]
    CM --> ALERT[Alert<br/>Policies]
    CM --> RB[Runbooks]

    style CM fill:#4285f4
    style DASH fill:#34a853
    style ALERT fill:#ea4335
`} client:visible />

---

## Ğ§Ñ‚Ğ¾ Ğ¼Ñ‹ ÑƒĞ·Ğ½Ğ°Ğ»Ğ¸

1. **Cloud SQL Monitoring:** Built-in Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ + custom replication slot queries Ğ´Ğ»Ñ WAL bloat detection
2. **Debezium Metrics:** JMX ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚ Ñ‡ĞµÑ€ĞµĞ· Prometheus, GKE Managed Service Ğ´Ğ»Ñ auto-collection
3. **Pub/Sub Monitoring:** Backlog age, DLQ count, publish rate Ğ´Ğ»Ñ early detection
4. **Dataflow Metrics:** System lag, worker count, cost tracking Ğ´Ğ»Ñ performance Ğ¸ budget
5. **Cloud Run Monitoring:** Error rate, latency distribution Ğ´Ğ»Ñ consumer health
6. **Unified Dashboard:** Single pane of glass Ğ´Ğ»Ñ Ğ²ÑĞµÑ… ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ² pipeline
7. **Alerting Policies:** CRITICAL/WARNING hierarchy Ñ runbooks Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ñ€ĞµĞ°ĞºÑ†Ğ¸Ğ¸
8. **Runbooks:** Step-by-step troubleshooting guides Ğ´Ğ»Ñ Ñ‚Ğ¸Ğ¿Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼

## Ğ§Ñ‚Ğ¾ Ğ´Ğ°Ğ»ÑŒÑˆĞµ?

Ğ’Ñ‹ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ¸Ğ»Ğ¸ **Module 6 - Cloud-Native GCP**! Ğ¡Ğ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ ÑˆĞ°Ğ³ â€” **Capstone Project**, Ğ³Ğ´Ğµ Ğ²Ñ‹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚Ğµ Ğ²ÑĞµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ production-ready CDC pipeline Ğ¾Ñ‚ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ´Ğ¾ ĞºĞ¾Ğ½Ñ†Ğ°.

**ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ Module 6:**

- Cloud SQL logical replication setup
- Debezium Server Kafka-less architecture
- IAM Ğ¸ Workload Identity security
- BigQuery replication Ñ‡ĞµÑ€ĞµĞ· Dataflow
- Event-driven processing Ñ Cloud Run
- End-to-end monitoring Ğ¸ observability

Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ñ‹ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ ÑÑ‚Ğ¾ Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞµ?
